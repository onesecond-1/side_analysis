{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "800958b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oracle_48/miniconda3/envs/ai-study/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/oracle_48/miniconda3/envs/ai-study/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[LogisticRegression] risk_score=IN\n",
      "Balanced Acc: 0.18290496543291232\n",
      "Macro F1     : 0.09937182424436915\n",
      "\n",
      "[Classification Report]\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.07      0.19      0.10      1596\n",
      "           1       0.31      0.11      0.16      6369\n",
      "           2       0.00      0.12      0.00        24\n",
      "           3       0.62      0.14      0.23     11955\n",
      "           4       0.00      0.36      0.00        56\n",
      "\n",
      "    accuracy                           0.13     20000\n",
      "   macro avg       0.20      0.18      0.10     20000\n",
      "weighted avg       0.48      0.13      0.19     20000\n",
      "\n",
      "[Confusion Matrix]\n",
      " [[ 297  154  216  180  749]\n",
      " [1241  688  950  806 2684]\n",
      " [   3    3    3    4   11]\n",
      " [2610 1359 1784 1653 4549]\n",
      " [  10    8   11    7   20]]\n",
      "\n",
      "================================================================================\n",
      "[XGBoost] risk_score=IN\n",
      "Balanced Acc: 0.22948534008014804\n",
      "Macro F1     : 0.1933477338258374\n",
      "\n",
      "[Classification Report]\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.12      0.42      0.18      1596\n",
      "           1       0.32      0.18      0.23      6369\n",
      "           2       0.00      0.04      0.00        24\n",
      "           3       0.65      0.47      0.55     11955\n",
      "           4       0.00      0.04      0.00        56\n",
      "\n",
      "    accuracy                           0.37     20000\n",
      "   macro avg       0.22      0.23      0.19     20000\n",
      "weighted avg       0.50      0.37      0.42     20000\n",
      "\n",
      "[Confusion Matrix]\n",
      " [[ 663  301   37  467  128]\n",
      " [2008 1154  166 2560  481]\n",
      " [   6    5    1   10    2]\n",
      " [2987 2166  307 5660  835]\n",
      " [  17   11    1   25    2]]\n",
      "\n",
      "[Sample Risk Prob] (1 - P(0))\n",
      "0    0.718825\n",
      "1    0.905558\n",
      "2    0.551441\n",
      "3    0.546021\n",
      "4    0.821495\n",
      "dtype: float32\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "list.remove(x): x not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 167\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# 4) Run both: risk_score 포함 vs 제외\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[1;32m    166\u001b[0m run_experiment(use_risk_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 167\u001b[0m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_risk_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 77\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(use_risk_score)\u001b[0m\n\u001b[1;32m     75\u001b[0m use_features \u001b[38;5;241m=\u001b[39m FEATURES\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_risk_score:\n\u001b[0;32m---> 77\u001b[0m     \u001b[43muse_features\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremove\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdiabetes_risk_score\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m X \u001b[38;5;241m=\u001b[39m dfr[use_features]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     80\u001b[0m y \u001b[38;5;241m=\u001b[39m df[TARGET]\u001b[38;5;241m.\u001b[39mcopy()\n",
      "\u001b[0;31mValueError\u001b[0m: list.remove(x): x not in list"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, balanced_accuracy_score, f1_score\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# (옵션) XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# =========================\n",
    "# 1) Load\n",
    "# =========================\n",
    "df = pd.read_csv(\"diabetes_dataset.csv\")\n",
    "\n",
    "FEATURES = [\n",
    "    \"smoking_status\",\n",
    "    \"alcohol_consumption_per_week\",\n",
    "    \"physical_activity_minutes_per_week\",\n",
    "    \"diet_score\",\n",
    "    \"sleep_hours_per_day\",\n",
    "    \"screen_time_hours_per_day\",\n",
    "    \"bmi\"\n",
    "]\n",
    "\n",
    "# diabetes_stage → 0~4로 세분화\n",
    "mapping = {\n",
    "    'No Diabetes': 0,\n",
    "    'Pre-Diabetes': 1,\n",
    "    'Type 1': 2,\n",
    "    'Type 2': 3,\n",
    "    'Gestational': 4\n",
    "}\n",
    "df['diabetes_stage_class'] = df['diabetes_stage'].map(mapping)\n",
    "\n",
    "TARGET = \"diabetes_stage_class\"\n",
    "\n",
    "# =========================\n",
    "# 2) Helper: preprocess + eval\n",
    "# =========================\n",
    "cat_cols = [\"smoking_status\"]\n",
    "num_cols = [\n",
    "    \"alcohol_consumption_per_week\",\n",
    "    \"physical_activity_minutes_per_week\",\n",
    "    \"diet_score\",\n",
    "    \"sleep_hours_per_day\",\n",
    "    \"screen_time_hours_per_day\",\n",
    "    \"bmi\"\n",
    "]\n",
    "dfr = df[FEATURES] # 타깃 컬럼(diabetes_stage) 버리기\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "        (\"num\", StandardScaler(), num_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "def evaluate(y_true, y_pred, title=\"\"):\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(title)\n",
    "    print(\"Balanced Acc:\", balanced_accuracy_score(y_true, y_pred))\n",
    "    print(\"Macro F1     :\", f1_score(y_true, y_pred, average=\"macro\"))\n",
    "    print(\"\\n[Classification Report]\\n\", classification_report(y_true, y_pred))\n",
    "    print(\"[Confusion Matrix]\\n\", confusion_matrix(y_true, y_pred))\n",
    "\n",
    "# =========================\n",
    "# 3) Train/Test split (stratify 필수)\n",
    "# =========================\n",
    "def run_experiment(use_risk_score: bool):\n",
    "    use_features = FEATURES.copy()\n",
    "    if not use_risk_score:\n",
    "        use_features.remove(\"diabetes_risk_score\")\n",
    "\n",
    "    X = dfr[use_features].copy()\n",
    "    y = df[TARGET].copy()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # 전처리 컬럼도 맞춰서 재정의\n",
    "    _cat_cols = [\"smoking_status\"]\n",
    "    _num_cols = [c for c in use_features if c != \"smoking_status\"]\n",
    "\n",
    "    _preprocess = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), _cat_cols),\n",
    "            (\"num\", StandardScaler(), _num_cols),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # A) 기준모델: 다항 로지스틱 회귀 (해석/베이스라인)\n",
    "    # -------------------------\n",
    "    logit = LogisticRegression(\n",
    "        multi_class=\"multinomial\",\n",
    "        solver=\"saga\",\n",
    "        max_iter=5000,\n",
    "        n_jobs=-1,\n",
    "        class_weight=\"balanced\",  # 불균형 대응\n",
    "    )\n",
    "\n",
    "    logit_clf = Pipeline([(\"prep\", _preprocess), (\"model\", logit)])\n",
    "    logit_clf.fit(X_train, y_train)\n",
    "    pred_logit = logit_clf.predict(X_test)\n",
    "\n",
    "    evaluate(\n",
    "        y_test, pred_logit,\n",
    "        title=f\"[LogisticRegression] risk_score={'IN' if use_risk_score else 'OUT'}\"\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # B) 주력모델: XGBoost (성능용)\n",
    "    #    - 불균형은 sample_weight로 대응\n",
    "    # -------------------------\n",
    "    # XGBoost는 스케일링이 필수는 아니지만(트리 기반),\n",
    "    # 파이프라인 단순화를 위해 동일 전처리 사용(원핫은 필요)\n",
    "    xgb = XGBClassifier(\n",
    "        objective=\"multi:softprob\",\n",
    "        eval_metric=\"mlogloss\",\n",
    "        n_estimators=600,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=4,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_lambda=1.0,\n",
    "        tree_method=\"hist\",\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    xgb_clf = Pipeline([(\"prep\", _preprocess), (\"model\", xgb)])\n",
    "\n",
    "    # 클래스 불균형 보정 가중치\n",
    "    sw = compute_sample_weight(class_weight=\"balanced\", y=y_train)\n",
    "\n",
    "    xgb_clf.fit(X_train, y_train, model__sample_weight=sw)\n",
    "    pred_xgb = xgb_clf.predict(X_test)\n",
    "\n",
    "    evaluate(\n",
    "        y_test, pred_xgb,\n",
    "        title=f\"[XGBoost] risk_score={'IN' if use_risk_score else 'OUT'}\"\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # (옵션) “당뇨 위험도” 확률로 뽑기 예시\n",
    "    #   - 예: No Diabetes가 아닌 확률을 위험도로 정의\n",
    "    # -------------------------\n",
    "    proba = xgb_clf.predict_proba(X_test)\n",
    "    classes = xgb_clf.named_steps[\"model\"].classes_\n",
    "    if 0 in classes:\n",
    "        idx_no = np.where(classes == 0)[0][0]\n",
    "        risk = 1.0 - proba[:, idx_no]\n",
    "        print(\"\\n[Sample Risk Prob] (1 - P(0))\")\n",
    "        print(pd.Series(risk).head())\n",
    "\n",
    "# =========================\n",
    "# 4) Run both: risk_score 포함 vs 제외\n",
    "# =========================\n",
    "run_experiment(use_risk_score=True)\n",
    "run_experiment(use_risk_score=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
